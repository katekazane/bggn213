---
title: "Class 8"
author: "Katelynn Kazane"
date: "2/6/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

k-Means

Basic formula
kmeans (x, centers=, nstart =)

```{r}
tmp <- c(rnorm(30,-3), rnorm(30,3))
x <- cbind(x=tmp, y=rev(tmp))

plot(x)
```

```{r}
km <- kmeans(x, centers= 2, nstart = 20 )
km
```

Size of clusters
```{r}
km$size
```

Cluster membership
```{r}
km$cluster
```

Visualization of the categories, and plotting of the center points
```{r}
plot(x, col=km$cluster)

#this following line is how we visualize where the centers are. 
points(km$centers, col = "blue", pch = 15, cex = 1)
```

First we need to calculate point (dis)similarity as the Euclidean distance between observations

```{r}
# First we need to calculate point (dis)similarity
# as the Euclidean distance between observations
dist_matrix <- dist(x)
# The hclust() function returns a hierarchical
# clustering model
hc <- hclust(d = dist_matrix)
# the print method is not so useful here
hc
```

```{r}
plot(hc)
abline(h=6, col = "red")
grp2 <- cutree(hc, h=6)

```

```{r}
plot(x, col = grp2)
```

```{r}
plot(hc)
abline(h = 2.5, col = "blue")
grp6 <- cutree(hc, h = 2.5)
table(grp6)
```

We can also use k = groups as an argument to cutree()
```{r}
plot(hc)
abline(h=6, col="red")
cutree(hc, k=2 ) 
```

Using different hierarchical clustering/linkage methods.
```{r}
d <- dist_matrix

hc.complete <- hclust(d, method="complete")
plot(hc.complete)

hc.average <- hclust(d, method="average")
plot(hc.average)

hc.single <- hclust(d, method="single")
plot(hc.single)
```

Made up overlapping data. This is slightly more realistic. 
```{r}
# Step 1. Generate some example data for clustering
x2 <- rbind(
 matrix(rnorm(100, mean=0, sd = 0.3), ncol = 2), # c1
 matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2), # c2
 matrix(c(rnorm(50, mean = 1, sd = 0.3), # c3
 rnorm(50, mean = 0, sd = 0.3)), ncol = 2))
colnames(x2) <- c("x", "y")
# Step 2. Plot the data without clustering
plot(x2)
# Step 3. Generate colors for known clusters
# (just so we can compare to hclust results)
col <- as.factor( rep(c("c1","c2","c3"), each=50) )
plot(x2, col=col)
```

Using this data, we are going to run some examples. Using the dist(), hclust(), plot() and cutree()  functions to return 2 and 3 clusters.

```{r}
dist_matrix2 <- dist(x2)
hc <- hclust(d = dist_matrix2)
plot(hc)
abline(hc, h = 2, col = "red")
cut2 <- cutree(hc, k = 2)
plot(x2, col = cut2)
```

```{r}
cut3 <- cutree(hc, k = 3)
plot(x2, col = cut3)
```

The same points are colored in different groups- so be careful with how we are grouping everything. 


A guide to PCA! Using real data.
```{r}
mydata <- read.csv("https://tinyurl.com/expression-CSV", row.names=1)
head(mydata)
```

**NOTE: prcomp() expects the samples to be rows and genes to be columns so we need to first transpose the matrix with the t() function!**

```{r}
head( t(mydata))
```

```{r}
## lets do PCA
pca <- prcomp(t(mydata), scale=TRUE)
summary(pca)
```

Making our first PCA plot
```{r}
#dim(pca$x)
plot(pca$x[,1], pca$x[,2], xlab = "PC1", ylab = "PC2")

```

```{r}
## Precent variance is often more informative to look at
pca.var <- pca$sdev^2
round(pca.var/sum(pca.var)*100, 1)
#pca.var.per <- round(pca.var/sum(pca.var)*100, 1)

pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
```

```{r}
barplot(pca.var.per, main="Scree Plot", xlab="Principal Component", ylab="Percent Variation")
```

Now we're making our PCA colorful and nice looking. 

```{r}
## A vector of colors for wt and ko samples
colvec <- as.factor( substr( colnames(mydata), 1, 2) )

plot(pca$x[,1], pca$x[,2], col=colvec, pch=16,
     xlab=paste0("PC1 (", pca.var.per[1], "%)"),
     ylab=paste0("PC2 (", pca.var.per[2], "%)"))
```


**Using real world examples for this**
PCA of UK Food Data

```{r}
uk <- read.csv("data/UK_foods.csv")
nrow(uk)
ncol(uk)
```
```{r}
head(uk)
```
```{r}
uk[,1]
```
```{r}
#nice way to exclude columns is to use negatives: -1 removes 1st column.
uk[,-1]
```

Assign row names from the first col of the data upon reading. This is a much safer approach. 
```{r}
read.csv("data/UK_foods.csv", row.names = 1)
```
To find numbers of rows and columns at once (not nrow() and ncol()- use dimension).
```{r}
dim(uk)
```

```{r}
rownames(x) <- x[,1]
x3 <- x[,-1]
head(x3)
```


```{r}
barplot(as.matrix(x3), beside=T, col=rainbow(nrow(x)))
```

Change the "beside" optional function to rearrange the charts. 
```{r}
barplot(as.matrix(x3), beside=F, col=rainbow(nrow(x)))

```

Now a direct comparison, using dot plots. 
```{r}
pairs(x, col=rainbow(10), pch=16)
```

##Using PCA to make everything more clear! 
```{r}
# Use the prcomp() PCA function 
pca <- prcomp( t(x3) )
summary(pca)
```

Making the PCA plot
```{r}
plot(pca$x[,1], pca$y[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$y[,2], colnames(x), col = c("orange", "red", "blue", "dark green"))

```

Now let's get weird! Focusing everything around PC1

```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```
